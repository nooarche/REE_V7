The V7 architecture provides a rigorous, mechanistic, and mathematically grounded theory for the **emergent nature of language**, positioning it not as a separate module, but as a **globally stable fixed point** arising from the system's fundamental local labelling dynamics and drive for computational compression. Language emerges as the **externalized reflective structure derived from stable internal labelling**.

The entire mechanism is built on the realization that if local constraint-following is possible within a network, then a global structure (like an ontology or language) emerges automatically, coordinating modules without central control.

Here is a detailed, stage-by-stage account of language emergence within the V7 framework:

I. Foundational Principle: Local Labelling and Global Consistency

The V7 architecture models its low-level perceptual substrate as a **graph** G where local modules (nodes) assign labels to incoming data. A core insight, drawn from theoretical mathematics, is that the local ability to classify neighborhoods in a stable, constraint-respecting way **is itself evidence of an underlying shared mathematical structure**.

**The Core Claim:** If distributed modules in V7 can perform **stable local labelling (colouring)** of their shared perceptual space, then **language is the global fixed-point of those local labelling dynamics**.

This approach is architecturally special because it implies that semantics crystallize automatically out of the system’s dynamics without assuming pre-existing semantics or forcing alignment from the top down.

II. Stage-by-Stage Emergence

Language is built through a progression involving object recognition, reference, and the compression of relational structures.

Stage 1: Perceptual Modules Create Proto-Labels (Local Colours)

In the initial phase, W-loops—particularly the Structural World Model (W1​)—identify patterns, generating simple labels or **"proto-categories"**.

• Each module checks for conflicts with neighbors and updates its labels via prediction error.

• A label stabilized across a neighborhood becomes a **semantic attractor**, representing the first pre-linguistic meaning.

Stage 2: Object Permanence and Global Invariants

The process of local labeling stability naturally leads to the emergence of internal **objects**.

• **Definition of Object Permanence (V7):** An internal object is an equivalence class of perceptual trajectories (γ) under **asymptotic label agreement**. The system exhibits object permanence when labels for a large class of trajectories corresponding to the same distal cause converge almost surely to the same stable value.

• As modules exchange inferences through the **shared latent workspace** (L-spaces), redundant labels collapse, and the labels that persist over many neighborhoods become **global invariants**. These invariants are the system's proto-word-like semantic clusters.

Stage 3: Emergent Reference (Noticing Correlations)

This stage fulfills the condition that "Noticing correlations between signals and internal representations begins the process".

• The system begins to associate externally observable **emissions** (st​), such as motor outputs or vocalizations, with particular stable internal labels or objects (c).

• The emission policy is implicitly optimized to reduce prediction error about others’ behavior.

• **Emergent Reference** occurs when certain emissions acquire **non-zero mutual information** (I) with specific object-classes. These emissions now "stand for" objects because using them helps reduce error.

Stage 4: Relational Structure Builds Proto-Syntax

This addresses the idea that "Relationships build syntax".

• The system learns **relational predicates** (R^j​)—spatial, temporal, causal, or role-based—among the set of internal objects (O).

• The system develops a **symbolic code (**Σ**)** whose structure minimizes a cost functional (J) that balances compression (Lcode​) against predictive fidelity (Lpred​) over sequences of object configurations and relations.

• **Proto-syntax** emerges because the cheapest way to encode relational configurations among persistent objects is to develop **compositional patterns over symbols**. This forces the emergence of syntactic rules that reflect the underlying relational structure {R^j​}.

Stage 5: Language as the Globally Stable Fixed Point

Full language emerges when the reflective layer constructs compressed symbolic codes.

• The **reflective layer** (L₃, supported by the DMN loop) finds that a short symbol referring to a stable attractor is cheaper than re-deriving the attractor every time.

• Language is the compression of stable labels, agency relations, abstract categories, and temporal structure.

• When perceptual, cross-module, reflective, and communal predictions all stabilize, **language appears as the globally self-consistent, minimal-energy coding of all these constraints**.

III. Supporting Architectural Context

**Social Contingency and Development (W₈):** The emergence of language is fundamentally tied to social interaction.

• The self-model and other-model (W8​) are necessary to bind labels to agents. Labels acquire **intentionality, agency sensitivity, and communicative affordance** when the system realizes that its labels and those of others have predictive power about each other’s behavior.

• Empirical evidence supports that **social contingency** (prompt and meaningful back-and-forth exchanges) is a powerful feature of the early language environment. Studies show that contingent interactions are crucial for toddlers to learn novel verbs, even over video chat, demonstrating that **time-locked bidirectional prediction** (a core V7 mechanism) is key to social learning and language.

• Developmentally, the **Symbolic/Narrative World Model (**W7​**)** emerges during childhood, using language to create rules, concepts, and narratives, updating the self-and-other concepts.

**The Reflective Structure (DMN):** Language is modeled as a higher-order reflective externalization of internal states.

• The **Symbolic/Normative World Model (**W7​**)**, residing in the language network and prefrontal cortex, deals with models of the world built from language, culture, and rules, providing **counterfactuals** that the brain cannot simulate otherwise.

• The entire system of semantics, language, shared meaning, and Theory of Mind (ToM) **all emerge from the same underlying labelling dynamics**. This gives V7 a mathematically defensible explanation of how language is built out of module dynamics.

In summary, the V7 architecture frames language emergence as a **structural necessity**, a result of the collective local predictive loops seeking optimal coherence and compression across a shared, multi-agent environment.
